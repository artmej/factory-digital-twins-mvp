---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-inference-config
data:
  MODEL_PATH: "/models"
  REDIS_URL: "redis://redis-service:6379"
  INFLUX_URL: "http://influxdb-service:8086"
  INFLUX_DB: "smart_factory_metrics"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  labels:
    app: ml-inference
    tier: ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
        tier: ai
    spec:
      containers:
      - name: ml-inference
        image: python:3.9-slim
        ports:
        - containerPort: 8080
        envFrom:
        - configMapRef:
            name: ml-inference-config
        command:
        - /bin/bash
        - -c
        - |
          pip install fastapi uvicorn scikit-learn redis influxdb-client numpy pandas
          cat > /app/main.py << 'EOF'
          from fastapi import FastAPI, HTTPException
          import numpy as np
          import redis
          import json
          import logging
          from influxdb import InfluxDBClient
          from datetime import datetime
          
          app = FastAPI(title="Smart Factory ML Inference API")
          
          # Initialize connections
          redis_client = redis.Redis(host='redis-service', port=6379, decode_responses=True)
          influx_client = InfluxDBClient(host='influxdb-service', port=8086, database='smart_factory_metrics')
          
          @app.get("/health")
          def health_check():
              return {"status": "healthy", "service": "ml-inference", "timestamp": datetime.utcnow()}
          
          @app.post("/predict/anomaly")
          def detect_anomaly(data: dict):
              try:
                  sensor_data = data.get('sensor_values', [])
                  machine_id = data.get('machine_id', 'unknown')
                  
                  # Simple anomaly detection (z-score based)
                  if len(sensor_data) > 10:
                      values = np.array(sensor_data)
                      z_scores = np.abs((values - np.mean(values)) / np.std(values))
                      anomaly_threshold = 2.0
                      anomalies = (z_scores > anomaly_threshold).tolist()
                      
                      result = {
                          "machine_id": machine_id,
                          "anomaly_detected": any(anomalies),
                          "anomaly_score": float(np.max(z_scores)),
                          "timestamp": datetime.utcnow().isoformat()
                      }
                      
                      # Cache result
                      redis_client.setex(f"anomaly:{machine_id}", 300, json.dumps(result))
                      
                      # Log to InfluxDB
                      point = {
                          "measurement": "anomaly_detection",
                          "tags": {"machine_id": machine_id},
                          "fields": {
                              "anomaly_score": result["anomaly_score"],
                              "anomaly_detected": result["anomaly_detected"]
                          },
                          "time": datetime.utcnow()
                      }
                      influx_client.write_points([point])
                      
                      return result
                  else:
                      return {"error": "Insufficient data for anomaly detection"}
              except Exception as e:
                  raise HTTPException(status_code=500, detail=str(e))
          
          @app.post("/predict/maintenance")
          def predict_maintenance(data: dict):
              try:
                  machine_id = data.get('machine_id')
                  runtime_hours = data.get('runtime_hours', 0)
                  vibration = data.get('vibration', 0)
                  temperature = data.get('temperature', 0)
                  
                  # Simple maintenance prediction logic
                  maintenance_score = 0
                  
                  if runtime_hours > 1000:
                      maintenance_score += 0.3
                  if vibration > 5.0:
                      maintenance_score += 0.4
                  if temperature > 80:
                      maintenance_score += 0.3
                  
                  needs_maintenance = maintenance_score > 0.7
                  
                  result = {
                      "machine_id": machine_id,
                      "maintenance_score": maintenance_score,
                      "needs_maintenance": needs_maintenance,
                      "recommended_action": "Schedule maintenance" if needs_maintenance else "Continue operation",
                      "timestamp": datetime.utcnow().isoformat()
                  }
                  
                  # Cache result
                  redis_client.setex(f"maintenance:{machine_id}", 300, json.dumps(result))
                  
                  return result
              except Exception as e:
                  raise HTTPException(status_code=500, detail=str(e))
          
          @app.get("/cache/{machine_id}")
          def get_cached_prediction(machine_id: str):
              anomaly_data = redis_client.get(f"anomaly:{machine_id}")
              maintenance_data = redis_client.get(f"maintenance:{machine_id}")
              
              return {
                  "anomaly": json.loads(anomaly_data) if anomaly_data else None,
                  "maintenance": json.loads(maintenance_data) if maintenance_data else None
              }
          EOF
          
          cd /app && uvicorn main:app --host 0.0.0.0 --port 8080
        workingDir: /app
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  labels:
    app: ml-inference
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 30002
    protocol: TCP
  selector:
    app: ml-inference