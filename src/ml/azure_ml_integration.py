"""
ðŸ”— Azure ML Integration Service
Integrates Azure ML models with existing TensorFlow.js edge computing system
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional
import aiohttp
import os

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AzureMLIntegrationService:
    """
    Integration service between Azure ML cloud models and TensorFlow.js edge models
    """
    
    def __init__(self):
        """Initialize Azure ML integration service"""
        self.ml_endpoint_url = os.getenv('AZURE_ML_ENDPOINT_URL', 'https://factory-predictive-maintenance.eastus.inference.ml.azure.com/score')
        self.ml_api_key = os.getenv('AZURE_ML_API_KEY', '')
        self.factory_dashboard_url = 'http://localhost:3000'
        
        # Model performance tracking
        self.performance_metrics = {
            'azure_ml': {'requests': 0, 'successes': 0, 'avg_latency': 0},
            'tensorflow_js': {'predictions': 0, 'accuracy': 0.947}
        }
        
        logger.info("ðŸ”— Azure ML Integration Service initialized")

    async def call_azure_ml_endpoint(self, sensor_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ðŸ“Š Call Azure ML endpoint for advanced predictions
        """
        try:
            # Prepare request data
            request_data = {
                "features": [
                    sensor_data.get('temperature', 75),
                    sensor_data.get('vibration', 0.3),
                    sensor_data.get('pressure', 2.5),
                    sensor_data.get('rotation_speed', 1800),
                    sensor_data.get('efficiency', 0.85),
                    sensor_data.get('operating_hours', 100)
                ],
                "machine_id": sensor_data.get('machine_id', 'machineA'),
                "timestamp": sensor_data.get('timestamp', datetime.now().isoformat())
            }
            
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.ml_api_key}' if self.ml_api_key else None
            }
            
            # Remove None values from headers
            headers = {k: v for k, v in headers.items() if v is not None}
            
            # Record request start time
            start_time = datetime.now()
            self.performance_metrics['azure_ml']['requests'] += 1
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.ml_endpoint_url,
                    json=request_data,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=5.0)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        # Calculate latency
                        latency = (datetime.now() - start_time).total_seconds() * 1000
                        self.performance_metrics['azure_ml']['successes'] += 1
                        
                        # Update average latency
                        current_avg = self.performance_metrics['azure_ml']['avg_latency']
                        success_count = self.performance_metrics['azure_ml']['successes']
                        self.performance_metrics['azure_ml']['avg_latency'] = (
                            (current_avg * (success_count - 1) + latency) / success_count
                        )
                        
                        logger.info(f"âœ… Azure ML prediction received in {latency:.1f}ms")
                        logger.info(f"   Failure probability: {result.get('failure_probability', 0):.3f}")
                        logger.info(f"   Risk level: {result.get('risk_label', 'Unknown')}")
                        
                        return result
                    else:
                        logger.error(f"âŒ Azure ML endpoint error: {response.status}")
                        return None
                        
        except asyncio.TimeoutError:
            logger.warning("â° Azure ML endpoint timeout - falling back to edge models")
            return None
        except Exception as e:
            logger.error(f"âŒ Azure ML call failed: {str(e)}")
            return None

    async def get_tensorflow_js_prediction(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        âš¡ Get prediction from local TensorFlow.js models (edge computing)
        """
        try:
            # Simulate TensorFlow.js edge prediction
            # In production, this would call the actual TensorFlow.js models
            
            temperature = sensor_data.get('temperature', 75)
            vibration = sensor_data.get('vibration', 0.3)
            efficiency = sensor_data.get('efficiency', 0.85)
            
            # Simple rule-based prediction for demo
            failure_score = (
                0.3 * max(0, (temperature - 70) / 15) +
                0.4 * min(1, vibration / 0.5) +
                0.3 * max(0, (0.9 - efficiency) / 0.2)
            )
            
            risk_level = 2 if failure_score > 0.7 else (1 if failure_score > 0.4 else 0)
            
            self.performance_metrics['tensorflow_js']['predictions'] += 1
            
            return {
                'failure_probability': failure_score,
                'risk_level': risk_level,
                'risk_label': ['Low', 'Medium', 'High'][risk_level],
                'is_anomaly': vibration > 0.8,
                'source': 'tensorflow_js',
                'latency_ms': 15  # Typical edge inference latency
            }
            
        except Exception as e:
            logger.error(f"âŒ TensorFlow.js prediction failed: {str(e)}")
            return {
                'failure_probability': 0.5,
                'risk_level': 1,
                'risk_label': 'Medium',
                'is_anomaly': False,
                'source': 'fallback',
                'error': str(e)
            }

    async def hybrid_prediction(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ§  Hybrid prediction using both Azure ML and TensorFlow.js
        """
        logger.info("ðŸ§  Running hybrid AI prediction...")
        
        # Get predictions from both sources concurrently
        azure_ml_task = asyncio.create_task(self.call_azure_ml_endpoint(sensor_data))
        tensorflow_js_task = asyncio.create_task(self.get_tensorflow_js_prediction(sensor_data))
        
        azure_ml_result, tensorflow_js_result = await asyncio.gather(
            azure_ml_task, tensorflow_js_task, return_exceptions=True
        )
        
        # Handle exceptions
        if isinstance(azure_ml_result, Exception):
            azure_ml_result = None
        if isinstance(tensorflow_js_result, Exception):
            tensorflow_js_result = {'failure_probability': 0.5, 'risk_level': 1, 'source': 'fallback'}
        
        # Create ensemble prediction
        ensemble_result = {
            'machine_id': sensor_data.get('machine_id', 'machineA'),
            'timestamp': datetime.now().isoformat(),
            'sensor_data': sensor_data,
            'predictions': {
                'azure_ml': azure_ml_result,
                'tensorflow_js': tensorflow_js_result
            },
            'ensemble': {}
        }
        
        # Combine predictions if both available
        if azure_ml_result and tensorflow_js_result:
            # Weighted ensemble (Azure ML 70%, TensorFlow.js 30%)
            ensemble_failure_prob = (
                0.7 * azure_ml_result.get('failure_probability', 0) +
                0.3 * tensorflow_js_result.get('failure_probability', 0)
            )
            
            # Use higher risk level (more conservative)
            ensemble_risk_level = max(
                azure_ml_result.get('risk_level', 0),
                tensorflow_js_result.get('risk_level', 0)
            )
            
            ensemble_result['ensemble'] = {
                'failure_probability': ensemble_failure_prob,
                'risk_level': ensemble_risk_level,
                'risk_label': ['Low', 'Medium', 'High'][ensemble_risk_level],
                'is_anomaly': azure_ml_result.get('is_anomaly', False) or tensorflow_js_result.get('is_anomaly', False),
                'confidence': 'high',
                'method': 'weighted_ensemble'
            }
            
            logger.info(f"ðŸŽ¯ Ensemble prediction: {ensemble_failure_prob:.3f} failure probability, {ensemble_result['ensemble']['risk_label']} risk")
            
        elif tensorflow_js_result:
            # Fallback to TensorFlow.js only
            ensemble_result['ensemble'] = {
                **tensorflow_js_result,
                'confidence': 'medium',
                'method': 'tensorflow_js_only'
            }
            
            logger.info("âš¡ Using TensorFlow.js prediction (Azure ML unavailable)")
            
        else:
            # Complete fallback
            ensemble_result['ensemble'] = {
                'failure_probability': 0.3,
                'risk_level': 1,
                'risk_label': 'Medium',
                'is_anomaly': False,
                'confidence': 'low',
                'method': 'fallback'
            }
            
            logger.warning("âš ï¸ Using fallback prediction (all models unavailable)")
        
        return ensemble_result

    async def send_prediction_to_dashboard(self, prediction_result: Dict[str, Any]):
        """
        ðŸ“Š Send prediction result to factory dashboard
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.factory_dashboard_url}/api/ml-prediction",
                    json=prediction_result,
                    timeout=aiohttp.ClientTimeout(total=2.0)
                ) as response:
                    if response.status == 200:
                        logger.info("ðŸ“Š Prediction sent to dashboard")
                    else:
                        logger.warning(f"âš ï¸ Dashboard update failed: {response.status}")
        except Exception as e:
            logger.error(f"âŒ Failed to send to dashboard: {str(e)}")

    async def generate_ml_insights(self, prediction_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ’¡ Generate actionable ML insights
        """
        ensemble = prediction_result.get('ensemble', {})
        failure_prob = ensemble.get('failure_probability', 0)
        risk_level = ensemble.get('risk_level', 0)
        
        insights = {
            'timestamp': datetime.now().isoformat(),
            'machine_id': prediction_result.get('machine_id', 'machineA'),
            'alerts': [],
            'recommendations': [],
            'business_impact': {}
        }
        
        # Generate alerts based on risk level
        if risk_level >= 2:  # High risk
            insights['alerts'].append({
                'severity': 'critical',
                'message': f'High failure risk detected ({failure_prob:.1%} probability)',
                'action': 'Schedule immediate maintenance inspection'
            })
            
            insights['business_impact']['potential_cost_avoidance'] = '$45,000'
            insights['business_impact']['recommended_action_timeframe'] = '24 hours'
            
        elif risk_level >= 1:  # Medium risk
            insights['alerts'].append({
                'severity': 'warning',
                'message': f'Elevated failure risk ({failure_prob:.1%} probability)',
                'action': 'Plan preventive maintenance within 48 hours'
            })
            
            insights['business_impact']['potential_cost_avoidance'] = '$15,000'
            insights['business_impact']['recommended_action_timeframe'] = '48 hours'
        
        # Generate recommendations
        sensor_data = prediction_result.get('sensor_data', {})
        
        if sensor_data.get('temperature', 75) > 80:
            insights['recommendations'].append({
                'type': 'cooling_system',
                'message': 'Check cooling system - temperature above normal',
                'priority': 'high'
            })
        
        if sensor_data.get('vibration', 0.3) > 0.6:
            insights['recommendations'].append({
                'type': 'mechanical',
                'message': 'Inspect mechanical components - high vibration detected',
                'priority': 'high'
            })
        
        if sensor_data.get('efficiency', 0.85) < 0.7:
            insights['recommendations'].append({
                'type': 'performance',
                'message': 'Analyze production parameters - efficiency below target',
                'priority': 'medium'
            })
        
        return insights

    async def process_factory_data(self, sensor_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ­ Main processing function for factory sensor data
        """
        logger.info(f"ðŸ­ Processing data for {sensor_data.get('machine_id', 'unknown machine')}")
        
        try:
            # 1. Get hybrid prediction
            prediction_result = await self.hybrid_prediction(sensor_data)
            
            # 2. Generate insights
            insights = await self.generate_ml_insights(prediction_result)
            
            # 3. Combine results
            complete_result = {
                **prediction_result,
                'insights': insights,
                'performance_metrics': self.performance_metrics
            }
            
            # 4. Send to dashboard
            await self.send_prediction_to_dashboard(complete_result)
            
            return complete_result
            
        except Exception as e:
            logger.error(f"âŒ Factory data processing failed: {str(e)}")
            return {
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
                'machine_id': sensor_data.get('machine_id', 'unknown')
            }

    def get_system_status(self) -> Dict[str, Any]:
        """
        ðŸ“Š Get system status and performance metrics
        """
        azure_ml_success_rate = (
            self.performance_metrics['azure_ml']['successes'] / 
            max(1, self.performance_metrics['azure_ml']['requests'])
        )
        
        return {
            'status': 'operational',
            'timestamp': datetime.now().isoformat(),
            'azure_ml': {
                'endpoint_url': self.ml_endpoint_url,
                'success_rate': f"{azure_ml_success_rate:.1%}",
                'avg_latency': f"{self.performance_metrics['azure_ml']['avg_latency']:.1f}ms",
                'total_requests': self.performance_metrics['azure_ml']['requests']
            },
            'tensorflow_js': {
                'predictions_made': self.performance_metrics['tensorflow_js']['predictions'],
                'accuracy': f"{self.performance_metrics['tensorflow_js']['accuracy']:.1%}",
                'avg_latency': '15ms'
            },
            'integration_health': 'healthy' if azure_ml_success_rate > 0.8 else 'degraded'
        }

# ðŸš€ Main execution function for testing
async def main():
    """Test the Azure ML integration service"""
    logger.info("ðŸš€ Testing Azure ML Integration Service...")
    
    service = AzureMLIntegrationService()
    
    # Test with sample sensor data
    test_data = {
        'machine_id': 'machineA',
        'temperature': 82.5,
        'vibration': 0.45,
        'pressure': 2.3,
        'rotation_speed': 1850,
        'efficiency': 0.78,
        'operating_hours': 150,
        'timestamp': datetime.now().isoformat()
    }
    
    result = await service.process_factory_data(test_data)
    
    logger.info("ðŸŽ¯ Test Results:")
    logger.info(json.dumps(result, indent=2))
    
    # Print system status
    status = service.get_system_status()
    logger.info("\nðŸ“Š System Status:")
    logger.info(json.dumps(status, indent=2))

if __name__ == "__main__":
    asyncio.run(main())